{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "input_dim = 28 * 28 # pixel length and width of each image\n",
    "output_dim = nb_classes = 10\n",
    "batch_size = 128\n",
    "nb_epoch = 20\n",
    "\n",
    "x_train = x_train.reshape(60000, input_dim)\n",
    "x_test = x_test.reshape(10000, input_dim)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255 # normalize by dividing by maximum pixel value\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, nb_classes)\n",
    "y_test = to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_of_first = x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "\n",
    "plt.subplot(141)\n",
    "plt.imshow(x_train[123].reshape(28,28), cmap=\"gray\")\n",
    "plt.title(\"Label of the image: {}\".format(y_train[123]))\n",
    "\n",
    "plt.subplot(142)\n",
    "plt.imshow(x_train[124].reshape(28,28), cmap=\"gray\")\n",
    "plt.title(\"Label of the image: {}\".format(y_train[124]))\n",
    "\n",
    "plt.subplot(143)\n",
    "plt.imshow(x_train[125].reshape(28,28), cmap=\"gray\")\n",
    "plt.title(\"Label of the image: {}\".format(y_train[125]))\n",
    "\n",
    "plt.subplot(144)\n",
    "plt.imshow(x_train[126].reshape(28,28), cmap=\"gray\")\n",
    "plt.title(\"Label of the image: {}\".format(y_train[126]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) In this task, you'll implement several ANN models with different activation functions. Specifically:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement a three layer ANN model with 128, 64 and 10 neurons in the layers using tanh activation function for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002591AE73828> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002591AE73828> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "60000/60000 [==============================] - 2s 26us/sample - loss: 1.0004 - accuracy: 0.7588 - val_loss: 0.5858 - val_accuracy: 0.8611\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.5170 - accuracy: 0.8688 - val_loss: 0.4362 - val_accuracy: 0.8866\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.4203 - accuracy: 0.8874 - val_loss: 0.3765 - val_accuracy: 0.8997\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.3736 - accuracy: 0.8973 - val_loss: 0.3427 - val_accuracy: 0.9081\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 0.3445 - accuracy: 0.9033 - val_loss: 0.3205 - val_accuracy: 0.9134\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.3238 - accuracy: 0.9083 - val_loss: 0.3036 - val_accuracy: 0.9173\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.3077 - accuracy: 0.9125 - val_loss: 0.2909 - val_accuracy: 0.9197\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.2946 - accuracy: 0.9163 - val_loss: 0.2805 - val_accuracy: 0.9222\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.2835 - accuracy: 0.9191 - val_loss: 0.2704 - val_accuracy: 0.9235\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.2737 - accuracy: 0.9225 - val_loss: 0.2625 - val_accuracy: 0.9258\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.2650 - accuracy: 0.9246 - val_loss: 0.2549 - val_accuracy: 0.9277\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.2571 - accuracy: 0.9264 - val_loss: 0.2482 - val_accuracy: 0.9293\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.2497 - accuracy: 0.9283 - val_loss: 0.2418 - val_accuracy: 0.9305\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.2430 - accuracy: 0.9309 - val_loss: 0.2366 - val_accuracy: 0.9331\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.2367 - accuracy: 0.9322 - val_loss: 0.2306 - val_accuracy: 0.9344\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.2307 - accuracy: 0.9342 - val_loss: 0.2252 - val_accuracy: 0.9348\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.2249 - accuracy: 0.9360 - val_loss: 0.2205 - val_accuracy: 0.9367\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.2195 - accuracy: 0.9371 - val_loss: 0.2162 - val_accuracy: 0.9376\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.2144 - accuracy: 0.9390 - val_loss: 0.2109 - val_accuracy: 0.9387\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.2094 - accuracy: 0.9402 - val_loss: 0.2066 - val_accuracy: 0.9397\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2591ae72108>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=shape_of_first, activation='tanh'))\n",
    "model.add(Dense(64, activation='tanh'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=20, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement a three layer ANN model with 128, 64 and 10 neurons in the layers using sigmoid activation function for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002591AA1FCA8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002591AA1FCA8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 2.2918 - accuracy: 0.1736 - val_loss: 2.2519 - val_accuracy: 0.3337\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 2.2250 - accuracy: 0.3571 - val_loss: 2.1911 - val_accuracy: 0.4393\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 2.1550 - accuracy: 0.4547 - val_loss: 2.1065 - val_accuracy: 0.4809\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 2.0550 - accuracy: 0.5133 - val_loss: 1.9847 - val_accuracy: 0.5120\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 1.9155 - accuracy: 0.5427 - val_loss: 1.8230 - val_accuracy: 0.5567\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 1.7434 - accuracy: 0.5758 - val_loss: 1.6403 - val_accuracy: 0.6058\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 1.5645 - accuracy: 0.6191 - val_loss: 1.4658 - val_accuracy: 0.6449\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 1.4018 - accuracy: 0.6593 - val_loss: 1.3145 - val_accuracy: 0.6713\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 1.2619 - accuracy: 0.6903 - val_loss: 1.1854 - val_accuracy: 0.7130\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 1.1438 - accuracy: 0.7208 - val_loss: 1.0776 - val_accuracy: 0.7367\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 1.0445 - accuracy: 0.7441 - val_loss: 0.9865 - val_accuracy: 0.7608\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.9610 - accuracy: 0.7643 - val_loss: 0.9101 - val_accuracy: 0.7789\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.8902 - accuracy: 0.7809 - val_loss: 0.8457 - val_accuracy: 0.7919\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.8299 - accuracy: 0.7952 - val_loss: 0.7889 - val_accuracy: 0.8034\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.7781 - accuracy: 0.8066 - val_loss: 0.7416 - val_accuracy: 0.8162\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.7335 - accuracy: 0.8169 - val_loss: 0.6997 - val_accuracy: 0.8257\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.6948 - accuracy: 0.8252 - val_loss: 0.6633 - val_accuracy: 0.8344\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.6610 - accuracy: 0.8332 - val_loss: 0.6316 - val_accuracy: 0.8426\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.6313 - accuracy: 0.8403 - val_loss: 0.6034 - val_accuracy: 0.8472\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.6051 - accuracy: 0.8456 - val_loss: 0.5788 - val_accuracy: 0.8535\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25924f33048>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=shape_of_first, activation='sigmoid'))\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=20, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement a three layer ANN model with 128, 64 and 10 neurons in the layers using ReLU activation function for each layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x00000259265B20D8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x00000259265B20D8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "60000/60000 [==============================] - 2s 25us/sample - loss: 1.2610 - accuracy: 0.6819 - val_loss: 0.6144 - val_accuracy: 0.8533\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.5091 - accuracy: 0.8679 - val_loss: 0.4080 - val_accuracy: 0.8909\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.3939 - accuracy: 0.8920 - val_loss: 0.3464 - val_accuracy: 0.9036\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 0.3466 - accuracy: 0.9030 - val_loss: 0.3145 - val_accuracy: 0.9117\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.3185 - accuracy: 0.9102 - val_loss: 0.2925 - val_accuracy: 0.9177\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 0.2978 - accuracy: 0.9151 - val_loss: 0.2757 - val_accuracy: 0.9234\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.2816 - accuracy: 0.9200 - val_loss: 0.2631 - val_accuracy: 0.9258\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.2677 - accuracy: 0.9239 - val_loss: 0.2510 - val_accuracy: 0.9304\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.2557 - accuracy: 0.9275 - val_loss: 0.2418 - val_accuracy: 0.9331\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.2450 - accuracy: 0.9306 - val_loss: 0.2328 - val_accuracy: 0.9363\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.2350 - accuracy: 0.9338 - val_loss: 0.2266 - val_accuracy: 0.9362\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 0.2260 - accuracy: 0.9363 - val_loss: 0.2171 - val_accuracy: 0.9400\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.2176 - accuracy: 0.9384 - val_loss: 0.2120 - val_accuracy: 0.9412\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.2096 - accuracy: 0.9407 - val_loss: 0.2048 - val_accuracy: 0.9417\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 0.2023 - accuracy: 0.9427 - val_loss: 0.1974 - val_accuracy: 0.9442\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.1957 - accuracy: 0.9445 - val_loss: 0.1923 - val_accuracy: 0.9457\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.1891 - accuracy: 0.9462 - val_loss: 0.1872 - val_accuracy: 0.9479\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 0.1833 - accuracy: 0.9482 - val_loss: 0.1816 - val_accuracy: 0.9492\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.1776 - accuracy: 0.9499 - val_loss: 0.1772 - val_accuracy: 0.9494\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.1723 - accuracy: 0.9511 - val_loss: 0.1740 - val_accuracy: 0.9504\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25926598c88>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=shape_of_first, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=20, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the result of each model with each other. Which activation function did perform better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relu activation function performed the best with a 95% test accuracy and overfit the model the least. The model that performed second best was the tanh activation funcation with an accuracy of 93% and the 3rd best model was the sigmoid activation funcation having an accuracy of only 85%. These models had the same number of layers and nodes per layer to ensure the only variable being altered was the activation fuction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) In this task, you'll implement the ANN models specified below with the hinge loss function as their loss functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement a three layer ANN model with 128, 64 and 10 neurons in the layers using tanh activation function for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002592655D0D8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002592655D0D8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 1.0782 - accuracy: 0.1511 - val_loss: 1.0760 - val_accuracy: 0.2233\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 1.0736 - accuracy: 0.2899 - val_loss: 1.0701 - val_accuracy: 0.3537\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 1.0663 - accuracy: 0.3568 - val_loss: 1.0615 - val_accuracy: 0.3772\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 1.0575 - accuracy: 0.3813 - val_loss: 1.0526 - val_accuracy: 0.4147\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 1.0489 - accuracy: 0.4270 - val_loss: 1.0438 - val_accuracy: 0.4695\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 1.0405 - accuracy: 0.4839 - val_loss: 1.0352 - val_accuracy: 0.5151\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 1.0322 - accuracy: 0.5201 - val_loss: 1.0271 - val_accuracy: 0.5397\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 1.0246 - accuracy: 0.5394 - val_loss: 1.0200 - val_accuracy: 0.5579\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 2s 25us/sample - loss: 1.0180 - accuracy: 0.5583 - val_loss: 1.0138 - val_accuracy: 0.5761\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 1.0122 - accuracy: 0.5810 - val_loss: 1.0083 - val_accuracy: 0.5974\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 1.0070 - accuracy: 0.6054 - val_loss: 1.0031 - val_accuracy: 0.6237\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 1s 25us/sample - loss: 1.0020 - accuracy: 0.6291 - val_loss: 0.9983 - val_accuracy: 0.6475\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.9975 - accuracy: 0.6506 - val_loss: 0.9939 - val_accuracy: 0.6665\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 0.9935 - accuracy: 0.6658 - val_loss: 0.9900 - val_accuracy: 0.6796\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.9898 - accuracy: 0.6764 - val_loss: 0.9865 - val_accuracy: 0.6887\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.9866 - accuracy: 0.6841 - val_loss: 0.9835 - val_accuracy: 0.6976\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.9837 - accuracy: 0.6897 - val_loss: 0.9807 - val_accuracy: 0.7032\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.9811 - accuracy: 0.6939 - val_loss: 0.9781 - val_accuracy: 0.7091\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.9784 - accuracy: 0.7002 - val_loss: 0.9753 - val_accuracy: 0.7167\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 1s 25us/sample - loss: 0.9753 - accuracy: 0.7166 - val_loss: 0.9718 - val_accuracy: 0.7406\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25926c90148>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=shape_of_first, activation='tanh'))\n",
    "model.add(Dense(64, activation='tanh'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='sgd', loss='hinge', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=20, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement a three layer ANN model with 128, 64 and 10 neurons in the layers using sigmoid activation function for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002592A208D38> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002592A208D38> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 1.0800 - accuracy: 0.1009 - val_loss: 1.0800 - val_accuracy: 0.1006\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 1.0800 - accuracy: 0.1015 - val_loss: 1.0799 - val_accuracy: 0.1009\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 2s 26us/sample - loss: 1.0799 - accuracy: 0.1018 - val_loss: 1.0799 - val_accuracy: 0.1010\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 2s 25us/sample - loss: 1.0799 - accuracy: 0.1021 - val_loss: 1.0799 - val_accuracy: 0.1010\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 1.0799 - accuracy: 0.1021 - val_loss: 1.0799 - val_accuracy: 0.1010\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 1.0799 - accuracy: 0.1022 - val_loss: 1.0798 - val_accuracy: 0.1010\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 1.0798 - accuracy: 0.1022 - val_loss: 1.0798 - val_accuracy: 0.1010\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 1.0798 - accuracy: 0.1022 - val_loss: 1.0798 - val_accuracy: 0.1010\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 1.0798 - accuracy: 0.1022 - val_loss: 1.0798 - val_accuracy: 0.1010\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 1.0798 - accuracy: 0.1022 - val_loss: 1.0797 - val_accuracy: 0.1010\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 1.0797 - accuracy: 0.1022 - val_loss: 1.0797 - val_accuracy: 0.1010\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 1.0797 - accuracy: 0.1022 - val_loss: 1.0797 - val_accuracy: 0.1010\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 1.0797 - accuracy: 0.1022 - val_loss: 1.0796 - val_accuracy: 0.1010\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 1.0796 - accuracy: 0.1022 - val_loss: 1.0796 - val_accuracy: 0.1010\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 1.0796 - accuracy: 0.1022 - val_loss: 1.0796 - val_accuracy: 0.1010\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 1.0796 - accuracy: 0.1022 - val_loss: 1.0796 - val_accuracy: 0.1010\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 1.0796 - accuracy: 0.1022 - val_loss: 1.0795 - val_accuracy: 0.1010\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 1.0795 - accuracy: 0.1022 - val_loss: 1.0795 - val_accuracy: 0.1010\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 1.0795 - accuracy: 0.1022 - val_loss: 1.0795 - val_accuracy: 0.1010\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 1.0795 - accuracy: 0.1022 - val_loss: 1.0794 - val_accuracy: 0.1010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2592a21be08>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=shape_of_first, activation='sigmoid'))\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='sgd', loss='hinge', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=20, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement a three layer ANN model with 128, 64 and 10 neurons in the layers using ReLU activation function for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x00000259283229D8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x00000259283229D8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 1.0796 - accuracy: 0.1405 - val_loss: 1.0790 - val_accuracy: 0.1564\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 1.0783 - accuracy: 0.1663 - val_loss: 1.0775 - val_accuracy: 0.1840\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 1.0768 - accuracy: 0.1963 - val_loss: 1.0757 - val_accuracy: 0.2169\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 1.0748 - accuracy: 0.2251 - val_loss: 1.0733 - val_accuracy: 0.2414\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 1.0721 - accuracy: 0.2465 - val_loss: 1.0699 - val_accuracy: 0.2589\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 1.0683 - accuracy: 0.2643 - val_loss: 1.0654 - val_accuracy: 0.2816\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 1.0631 - accuracy: 0.2896 - val_loss: 1.0591 - val_accuracy: 0.3142\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 1.0560 - accuracy: 0.3185 - val_loss: 1.0513 - val_accuracy: 0.3422\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 1.0486 - accuracy: 0.3420 - val_loss: 1.0442 - val_accuracy: 0.3675\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 1.0421 - accuracy: 0.3769 - val_loss: 1.0381 - val_accuracy: 0.4039\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 1s 25us/sample - loss: 1.0366 - accuracy: 0.4132 - val_loss: 1.0328 - val_accuracy: 0.4440\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 2s 25us/sample - loss: 1.0316 - accuracy: 0.4478 - val_loss: 1.0279 - val_accuracy: 0.4752\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 2s 25us/sample - loss: 1.0269 - accuracy: 0.4780 - val_loss: 1.0232 - val_accuracy: 0.5011\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 2s 25us/sample - loss: 1.0223 - accuracy: 0.5009 - val_loss: 1.0185 - val_accuracy: 0.5188\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 1.0176 - accuracy: 0.5215 - val_loss: 1.0136 - val_accuracy: 0.5390\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 1.0124 - accuracy: 0.5569 - val_loss: 1.0079 - val_accuracy: 0.5864\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 1.0062 - accuracy: 0.6116 - val_loss: 1.0013 - val_accuracy: 0.6364\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 1.0001 - accuracy: 0.6451 - val_loss: 0.9955 - val_accuracy: 0.6616\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 2s 25us/sample - loss: 0.9947 - accuracy: 0.6628 - val_loss: 0.9904 - val_accuracy: 0.6755\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.9901 - accuracy: 0.6732 - val_loss: 0.9861 - val_accuracy: 0.6835\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25928b146c8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=shape_of_first, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='sgd', loss='hinge', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=20, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the result of each model with the result of the same model from the previous task. Which loss function did perform better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing the models with respect to hinge being their new common loss function the ranking is as follows: the tanh activation function scored the highest with a 74% test accuracy, relu scored the next highest with 68% test accuracy, and the sigmoid activation function performed the worst with only a 10% test accuracy.\n",
    "\n",
    "If we look at these three models' scores compared to the previous three it is easier to conclude that the categorical crossentropy loss function provided a higher test accuracy overall for this particular dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
