{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "input_dim = 28 * 28 # pixel length and width of each image\n",
    "output_dim = nb_classes = 10\n",
    "nb_epoch = 20\n",
    "\n",
    "x_train = x_train.reshape(60000, input_dim)\n",
    "x_test = x_test.reshape(10000, input_dim)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255 # normalize by dividing by maximum pixel value\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, nb_classes)\n",
    "y_test = to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_of_first = x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "\n",
    "plt.subplot(141)\n",
    "plt.imshow(x_train[123].reshape(28,28), cmap=\"gray\")\n",
    "plt.title(\"Label of the image: {}\".format(y_train[123]))\n",
    "\n",
    "plt.subplot(142)\n",
    "plt.imshow(x_train[124].reshape(28,28), cmap=\"gray\")\n",
    "plt.title(\"Label of the image: {}\".format(y_train[124]))\n",
    "\n",
    "plt.subplot(143)\n",
    "plt.imshow(x_train[125].reshape(28,28), cmap=\"gray\")\n",
    "plt.title(\"Label of the image: {}\".format(y_train[125]))\n",
    "\n",
    "plt.subplot(144)\n",
    "plt.imshow(x_train[126].reshape(28,28), cmap=\"gray\")\n",
    "plt.title(\"Label of the image: {}\".format(y_train[126]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) In this task, you'll implement several ANN models with different batch sizes. Specifically:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement a three layer ANN model with 128, 64 and 10 neurons in the layers using 8 as the mini batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000001369931C5E8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000001369931C5E8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "60000/60000 [==============================] - 15s 243us/sample - loss: 0.3699 - accuracy: 0.8951 - val_loss: 0.2099 - val_accuracy: 0.9398\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 14s 227us/sample - loss: 0.1772 - accuracy: 0.9475 - val_loss: 0.1417 - val_accuracy: 0.9580\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 14s 231us/sample - loss: 0.1267 - accuracy: 0.9636 - val_loss: 0.1153 - val_accuracy: 0.9658\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 14s 227us/sample - loss: 0.0982 - accuracy: 0.9712 - val_loss: 0.1008 - val_accuracy: 0.9690\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 14s 225us/sample - loss: 0.0803 - accuracy: 0.9757 - val_loss: 0.0910 - val_accuracy: 0.9723\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 14s 228us/sample - loss: 0.0675 - accuracy: 0.9796 - val_loss: 0.0879 - val_accuracy: 0.9736\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 14s 230us/sample - loss: 0.0585 - accuracy: 0.9823 - val_loss: 0.0860 - val_accuracy: 0.9727\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 14s 227us/sample - loss: 0.0499 - accuracy: 0.9858 - val_loss: 0.0793 - val_accuracy: 0.9746\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 14s 226us/sample - loss: 0.0430 - accuracy: 0.9871 - val_loss: 0.0798 - val_accuracy: 0.9746\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 14s 228us/sample - loss: 0.0374 - accuracy: 0.9894 - val_loss: 0.0755 - val_accuracy: 0.9762\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 14s 228us/sample - loss: 0.0323 - accuracy: 0.9911 - val_loss: 0.0776 - val_accuracy: 0.9747\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 14s 226us/sample - loss: 0.0283 - accuracy: 0.9919 - val_loss: 0.0798 - val_accuracy: 0.9762\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 14s 225us/sample - loss: 0.0243 - accuracy: 0.9931 - val_loss: 0.0741 - val_accuracy: 0.9784\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 14s 236us/sample - loss: 0.0210 - accuracy: 0.9945 - val_loss: 0.0743 - val_accuracy: 0.9781\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 15s 245us/sample - loss: 0.0185 - accuracy: 0.9951 - val_loss: 0.0742 - val_accuracy: 0.9772\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 14s 230us/sample - loss: 0.0156 - accuracy: 0.9964 - val_loss: 0.0765 - val_accuracy: 0.9793\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 13s 224us/sample - loss: 0.0138 - accuracy: 0.9968 - val_loss: 0.0785 - val_accuracy: 0.9771\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 14s 229us/sample - loss: 0.0113 - accuracy: 0.9979 - val_loss: 0.0824 - val_accuracy: 0.9752\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 14s 235us/sample - loss: 0.0098 - accuracy: 0.9981 - val_loss: 0.0787 - val_accuracy: 0.9775\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 14s 232us/sample - loss: 0.0084 - accuracy: 0.9987 - val_loss: 0.0786 - val_accuracy: 0.9780\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13699309bc8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=shape_of_first, activation ='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=8, epochs=20, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement a three layer ANN model with 128, 64 and 10 neurons in the layers using 128 as the mini batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x00000136995B5798> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x00000136995B5798> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 1.2442 - accuracy: 0.6886 - val_loss: 0.6150 - val_accuracy: 0.8539\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 0.5044 - accuracy: 0.8700 - val_loss: 0.4060 - val_accuracy: 0.8942\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 0.3861 - accuracy: 0.8946 - val_loss: 0.3424 - val_accuracy: 0.9053\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 0.3372 - accuracy: 0.9051 - val_loss: 0.3093 - val_accuracy: 0.9120\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 0.3085 - accuracy: 0.9127 - val_loss: 0.2887 - val_accuracy: 0.9189\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 0.2877 - accuracy: 0.9178 - val_loss: 0.2708 - val_accuracy: 0.9238\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 0.2717 - accuracy: 0.9229 - val_loss: 0.2591 - val_accuracy: 0.9274\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 0.2586 - accuracy: 0.9259 - val_loss: 0.2463 - val_accuracy: 0.9293\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.2470 - accuracy: 0.9303 - val_loss: 0.2370 - val_accuracy: 0.9329\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 0.2370 - accuracy: 0.9325 - val_loss: 0.2274 - val_accuracy: 0.9357\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 0.2278 - accuracy: 0.9352 - val_loss: 0.2215 - val_accuracy: 0.9367\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 0.2196 - accuracy: 0.9379 - val_loss: 0.2147 - val_accuracy: 0.9391\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.2121 - accuracy: 0.9400 - val_loss: 0.2088 - val_accuracy: 0.9400\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 0.2051 - accuracy: 0.9418 - val_loss: 0.2016 - val_accuracy: 0.9413\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 0.1983 - accuracy: 0.9442 - val_loss: 0.1962 - val_accuracy: 0.9429\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.1926 - accuracy: 0.9457 - val_loss: 0.1906 - val_accuracy: 0.9441\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 0.1868 - accuracy: 0.9477 - val_loss: 0.1866 - val_accuracy: 0.9445\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 0.1815 - accuracy: 0.9489 - val_loss: 0.1833 - val_accuracy: 0.9462\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.1765 - accuracy: 0.9504 - val_loss: 0.1788 - val_accuracy: 0.9468\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.1718 - accuracy: 0.9517 - val_loss: 0.1736 - val_accuracy: 0.9483\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1369b9f40c8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=shape_of_first, activation ='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement a three layer ANN model with 128, 64 and 10 neurons in the layers using the full sample as the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x00000135C8123438> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x00000135C8123438> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "60000/60000 [==============================] - 1s 12us/sample - loss: 2.1422 - accuracy: 0.2723 - val_loss: 1.9166 - val_accuracy: 0.4583\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 0s 6us/sample - loss: 1.7108 - accuracy: 0.5844 - val_loss: 1.4627 - val_accuracy: 0.6791\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 0s 6us/sample - loss: 1.2882 - accuracy: 0.7173 - val_loss: 1.0883 - val_accuracy: 0.7553\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 0s 6us/sample - loss: 0.9868 - accuracy: 0.7715 - val_loss: 0.8560 - val_accuracy: 0.8007\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 0s 6us/sample - loss: 0.8048 - accuracy: 0.8058 - val_loss: 0.7168 - val_accuracy: 0.8264\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 0s 6us/sample - loss: 0.6916 - accuracy: 0.8279 - val_loss: 0.6264 - val_accuracy: 0.8459\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 0s 6us/sample - loss: 0.6162 - accuracy: 0.8432 - val_loss: 0.5643 - val_accuracy: 0.8571\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 0s 6us/sample - loss: 0.5625 - accuracy: 0.8536 - val_loss: 0.5190 - val_accuracy: 0.8670\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 0s 6us/sample - loss: 0.5224 - accuracy: 0.8623 - val_loss: 0.4847 - val_accuracy: 0.8746\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 0s 6us/sample - loss: 0.4914 - accuracy: 0.8690 - val_loss: 0.4580 - val_accuracy: 0.8790\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 0s 6us/sample - loss: 0.4665 - accuracy: 0.8750 - val_loss: 0.4361 - val_accuracy: 0.8839\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 0s 6us/sample - loss: 0.4459 - accuracy: 0.8794 - val_loss: 0.4178 - val_accuracy: 0.8859\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 0s 6us/sample - loss: 0.4288 - accuracy: 0.8829 - val_loss: 0.4031 - val_accuracy: 0.8914\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 0s 6us/sample - loss: 0.4142 - accuracy: 0.8859 - val_loss: 0.3896 - val_accuracy: 0.8944\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 0s 6us/sample - loss: 0.4015 - accuracy: 0.8889 - val_loss: 0.3782 - val_accuracy: 0.8967\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 0s 6us/sample - loss: 0.3904 - accuracy: 0.8917 - val_loss: 0.3683 - val_accuracy: 0.8988\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 0s 6us/sample - loss: 0.3805 - accuracy: 0.8946 - val_loss: 0.3600 - val_accuracy: 0.8989\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 0s 6us/sample - loss: 0.3718 - accuracy: 0.8964 - val_loss: 0.3517 - val_accuracy: 0.9018\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 0s 6us/sample - loss: 0.3638 - accuracy: 0.8988 - val_loss: 0.3450 - val_accuracy: 0.9023\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 0s 6us/sample - loss: 0.3565 - accuracy: 0.9001 - val_loss: 0.3384 - val_accuracy: 0.9044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x136a3b28148>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=shape_of_first, activation ='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=x_train.shape[1], epochs=20, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the result of each model with each other. Which batch size did perform better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with the lowest batch size performed the best at 97% test accuracy, but took significantly longer at around 14sec per epoch. Both the runtime and accuracy are inversely scaled with batch size with the second model coming in second in both accuracy and runtime and the last model having the worst accuracy at 90%, but a significantly faster runtime at only 6 microseconds per epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) In this task, you'll implement several ANN models with different learning rates for the stochastic gradient descent. In all of the models below, use 128 as your mini batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement a three layer ANN model with 128, 64 and 10 neurons in the layers using 0.01 as the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x00000136A3A39438> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x00000136A3A39438> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 1.2807 - accuracy: 0.6712 - val_loss: 0.6076 - val_accuracy: 0.8521\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 0.5013 - accuracy: 0.8684 - val_loss: 0.4046 - val_accuracy: 0.8905\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 0.3860 - accuracy: 0.8935 - val_loss: 0.3419 - val_accuracy: 0.9051\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 0.3389 - accuracy: 0.9051 - val_loss: 0.3107 - val_accuracy: 0.9122\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 0.3106 - accuracy: 0.9118 - val_loss: 0.2909 - val_accuracy: 0.9176\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.2903 - accuracy: 0.9175 - val_loss: 0.2723 - val_accuracy: 0.9230\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.2739 - accuracy: 0.9217 - val_loss: 0.2583 - val_accuracy: 0.9269\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.2598 - accuracy: 0.9256 - val_loss: 0.2477 - val_accuracy: 0.9310\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.2482 - accuracy: 0.9292 - val_loss: 0.2368 - val_accuracy: 0.9335\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.2373 - accuracy: 0.9327 - val_loss: 0.2277 - val_accuracy: 0.9333\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 0.2274 - accuracy: 0.9349 - val_loss: 0.2201 - val_accuracy: 0.9372\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.2185 - accuracy: 0.9375 - val_loss: 0.2129 - val_accuracy: 0.9394\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 0.2104 - accuracy: 0.9405 - val_loss: 0.2054 - val_accuracy: 0.9394\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 0.2026 - accuracy: 0.9421 - val_loss: 0.1979 - val_accuracy: 0.9419\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 0.1956 - accuracy: 0.9440 - val_loss: 0.1919 - val_accuracy: 0.9435\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 0.1890 - accuracy: 0.9462 - val_loss: 0.1863 - val_accuracy: 0.9456\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 0.1826 - accuracy: 0.9483 - val_loss: 0.1805 - val_accuracy: 0.9465\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 0.1769 - accuracy: 0.9500 - val_loss: 0.1763 - val_accuracy: 0.9483\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.1714 - accuracy: 0.9516 - val_loss: 0.1719 - val_accuracy: 0.9485\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 0.1663 - accuracy: 0.9523 - val_loss: 0.1663 - val_accuracy: 0.9499\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x136a75bcfc8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = SGD(lr=.01)\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=shape_of_first, activation ='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement a three layer ANN model with 128, 64 and 10 neurons in the layers using 100 as the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x00000136A5EC4948> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x00000136A5EC4948> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 969556992.3459 - accuracy: 0.1141 - val_loss: 27488.5248 - val_accuracy: 0.1135\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 39.1187 - accuracy: 0.1144 - val_loss: 27485.2907 - val_accuracy: 0.1172\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 38.3596 - accuracy: 0.1163 - val_loss: 27484.8869 - val_accuracy: 0.1124\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 2s 26us/sample - loss: 38.4454 - accuracy: 0.1140 - val_loss: 27487.0823 - val_accuracy: 0.1135\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 38.7560 - accuracy: 0.1141 - val_loss: 27480.3379 - val_accuracy: 0.1176\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 39.3441 - accuracy: 0.1120 - val_loss: 27474.4396 - val_accuracy: 0.1176\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 38.8274 - accuracy: 0.1135 - val_loss: 27490.4489 - val_accuracy: 0.1101\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 39.0798 - accuracy: 0.1139 - val_loss: 27481.8802 - val_accuracy: 0.1124\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 2s 25us/sample - loss: 38.4454 - accuracy: 0.1142 - val_loss: 27493.2514 - val_accuracy: 0.1154\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 2s 25us/sample - loss: 40.0558 - accuracy: 0.1123 - val_loss: 27480.4681 - val_accuracy: 0.1126\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 1s 25us/sample - loss: 39.4864 - accuracy: 0.1137 - val_loss: 27487.7171 - val_accuracy: 0.1154\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 39.5944 - accuracy: 0.1154 - val_loss: 27486.6522 - val_accuracy: 0.1036\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 39.0354 - accuracy: 0.1160 - val_loss: 27481.9453 - val_accuracy: 0.1126\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 39.7224 - accuracy: 0.1127 - val_loss: 27487.4364 - val_accuracy: 0.1176\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 38.8026 - accuracy: 0.1141 - val_loss: 27481.0657 - val_accuracy: 0.1036\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 38.6228 - accuracy: 0.1152 - val_loss: 27482.3470 - val_accuracy: 0.1101\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 38.9645 - accuracy: 0.1116 - val_loss: 27481.1643 - val_accuracy: 0.1036\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 39.3692 - accuracy: 0.1140 - val_loss: 27486.5643 - val_accuracy: 0.1154\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 38.9389 - accuracy: 0.1151 - val_loss: 27480.8939 - val_accuracy: 0.1101\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 39.6827 - accuracy: 0.1133 - val_loss: 27477.0436 - val_accuracy: 0.1124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x136a5edc908>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = SGD(lr=100)\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=shape_of_first, activation ='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement a three layer ANN model with 128, 64 and 10 neurons in the layers using 0.0000001 as the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x00000136A5E5D8B8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x00000136A5E5D8B8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "60000/60000 [==============================] - 2s 26us/sample - loss: 2.3666 - accuracy: 0.0970 - val_loss: 2.3685 - val_accuracy: 0.0947\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 2.3666 - accuracy: 0.0970 - val_loss: 2.3684 - val_accuracy: 0.0947\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3665 - accuracy: 0.0970 - val_loss: 2.3684 - val_accuracy: 0.0947\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3665 - accuracy: 0.0970 - val_loss: 2.3684 - val_accuracy: 0.0947\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 2.3665 - accuracy: 0.0970 - val_loss: 2.3684 - val_accuracy: 0.0948\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 2.3665 - accuracy: 0.0970 - val_loss: 2.3684 - val_accuracy: 0.0948\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3665 - accuracy: 0.0970 - val_loss: 2.3684 - val_accuracy: 0.0948\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 2.3665 - accuracy: 0.0970 - val_loss: 2.3683 - val_accuracy: 0.0948\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 2.3664 - accuracy: 0.0970 - val_loss: 2.3683 - val_accuracy: 0.0948\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 2.3664 - accuracy: 0.0970 - val_loss: 2.3683 - val_accuracy: 0.0948\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3664 - accuracy: 0.0970 - val_loss: 2.3683 - val_accuracy: 0.0948\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 2.3664 - accuracy: 0.0970 - val_loss: 2.3683 - val_accuracy: 0.0948\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3664 - accuracy: 0.0970 - val_loss: 2.3683 - val_accuracy: 0.0948\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3664 - accuracy: 0.0970 - val_loss: 2.3682 - val_accuracy: 0.0948\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3664 - accuracy: 0.0971 - val_loss: 2.3682 - val_accuracy: 0.0949\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3663 - accuracy: 0.0971 - val_loss: 2.3682 - val_accuracy: 0.0949\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 2.3663 - accuracy: 0.0971 - val_loss: 2.3682 - val_accuracy: 0.0949\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 2.3663 - accuracy: 0.0971 - val_loss: 2.3682 - val_accuracy: 0.0949\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 2.3663 - accuracy: 0.0971 - val_loss: 2.3682 - val_accuracy: 0.0949\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 2.3663 - accuracy: 0.0971 - val_loss: 2.3681 - val_accuracy: 0.0949\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x136a757fc88>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = SGD(lr=.0000001)\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=shape_of_first, activation ='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the result of each model with each other. Which learning rate did perform better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The smaller the learning rate, the higher the test accuracy, but both the models with learning rates closest to zero were fairly close in accuracy. The model with the lowest learning rate had an accuracy of 95%, butthe model overfit to the training data slightly better than the second lowest learning rate (which also had a test accuracy of 95%). The model with a learning rate of 100 had a terrible 11% test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
